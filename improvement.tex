\tightsection{Improving Quality with GO}
\label{sec:improvement}

Previous sections have established our prediction algorithm can achieve high prediction accuracy. In this section, we empirically study if a good prediction lead to significant quality improvements. It should be noticed that the possible improvements are bounded by the actual difference between the performance of resources (e.g., CDN or bitrate). As pointed out in \Section~\ref{sec:intro}, current practices in the industry, only allow coarse grain decisions (e.g., inter-CDN selection) and the difference at such level is much less significant than finer-grain ones (e.g., server selection or path selection). Therefore, though our quantitative results based on our dataset do not show remarkable improvements under CDN or bitrate selection, we expect to see higher improvements in the future with more fine-grain selection enabled.

Specifically, we aim to answer three questions:
\begin{packedenumerate}
	\item How does GO select best decision based on prediction (\Section~\ref{subsec:behavior})?
	\item How much quality improvement can GO achieve (\Section~\ref{subsec:go-improve})?
	\item How much do different factors (e.g., prediction accuracy and quality diversity) impact quality improvement (\Section~\ref{subsec:impact-accuracy} and ~\ref{subsec:impact-diversity})?
\end{packedenumerate}

\tightsubsection{Methodology \jc{This part must be shrinked}}

We study these questions by using offline trace-driven analysis. While we have built and deployed GO in production (see \Section~\ref{sec:eval} for results), we believe these questions should be better addressed using offline trace-driven analysis, as it excells in several importnat dimensions:
\begin{packedenumerate}
	\item Higher overage: The real world experiments only have access to the video traffic of a single video site using a single type of OS and a single streaming protocol. %The greatly constraints its coverage on types of traffic.
	\item Higher scale of simultaneous expriments: Since the real world experiment only controls one site's traffic, it is infeasible to compare a large number of configurations or algorithms in parallel (in practice we can do at most 3-4) and ensure each of them receives sufficient traffic to get usable confident intervals.
	\item Higher diversity in performance: As we will see, quality improvement is greatly impacted by the diversity in the performance outcome among different decisions (e.g., different CDNs). However, given our deployment limitations, we can only observe a limited number of patterns or performance diversity in real world expreiments.
	\item More controlled experiments: In a real deployment is also impossible to conduct sensitivity analysis (e.g., the third question) where we would like to change one parameter (e.g., CDN performance) at a time in a controlled manner.
\end{packedenumerate}


This section uses three type of inputs to answer these questions. We next describe these types of inputs as the increasing order of their flexibility flexibility and the decreased order of their closeness to the real world.

\myparatight{Counterfactual input} This approach is the closest to
real-world experiment. The basic idea is that we first collect a
dataset in which the decisions are randomly made for each client
(called {\it random dataset}) and collect the quality metrics of each
video session under random decisions. Then we evaluate a given
decision algorithm by picking the sessions for which the decision made
by the algorithm \emph{matches} the decisions in the random
dataset. In~\cite{technicalreport}, we show that this approach is
unbiased.  By using this methodology, we are able to replay multiple
algorithms on the data sets.

\myparatight{Augmented trace-driven input} While counterfactual input
is useful in evaluating different algorithms, we cannot use it to
estimate the ``would-be" performance of an ``oracle" approach (e.g.,
how the performance would be if another decision were taken), as this
would require data extrapolation. To remedy these limitations, we
generate an augmented trace-driven synthetic dataset in which we
extrapolate for each session the ``true'' quality outcome of all its
possible decisions. The basic idea of this extrapolation is that for
each session $s$, its outcome of each decision $d$ is drawn from the
distribution of outcomes of this decision on other sessions exactly
matching the same attributes as $s$, i.e., in the same partition as
$s$. Having ensured that quality outcomes in the synthetic scenario
are known for any decision, it is possible to identify an {\it oracle}
approach that always makes the best decision. However, such evaluation
might be biased due to the extrapolation.

\myparatight{Controlled synthetic input} Both previous methodologies
have no support for creating new test scenarios. For example, if no
major outage or other events happen in the data set, we cannot
evaluate GO performance under those scenarios. Thus we also use fully
controlleable inputs where the true outcomes (i.e., ``ground truth'')
of each decision are controlled.


\tightsubsection{A Simple Example}
\label{subsec:behavior}

We first explain the GO's prediction-based decision algorithm and then
use controlled synthetic input to study its behavior. As expected, GO
identifies the performance changes and makes decisions to reflect
these changes.

\myparatight{Prediction-based dececisions} When making a decision, GO
simply selects for each session under prediction the decision that has
the best predicted quality in a certain metric. Note that GO always
leaves a small fraction of traffic to be randomly allocated to
guarantee that each decision will have at least certain fraction of
traffic even when that decision is not the best for every session at
this point (but potentially, the best in the future).

\begin{figure}[h!]
\centering
 \includegraphics[width=0.5\textwidth] {figures/behavior-evaluation/simple-change.pdf}
\tightcaption{Case for behavioral study}
\label{fig:behavioral}
\end{figure}

\myparatight{Case study} We simulate a scenario in which a sudden
change in performance of one group causes GO to change its decisions
accordingly. Figure~\ref{fig:behavioral} shows the behavior of
sessions in one ASN when provided with two CDNs to choose from. We
only use buffering ratio as quality metric. The mean of buffering
ratio of CDN1 is stably at 0.2 and that of CDN2 changes between 0.1
and 0.25, with a random interval. Each quality sample of a CDN is
generated so that its buffering ratio has a Guassian noise with
standard deviation of 0.01 from mean. This figure shows clearly that
under this configuration, GO is able to switch to the best CDN after
an expected delay (the quality metric is computed over a 30-minute
sliding window). Note that there is always a small amount of
randomized traffic on eeach CDN to make sure that we can measure the
outcomes of all possible decisions. 

%As a side-effect, it takes longer for GO to detect that CDN2 has bad bad quality than when it has good quality, because the number of samples from CDN2 when it has bad quality (only randomized traffic) is smaller than when it has good quality (all traffic).

%\xil{are we going to do something about that point or ack it is an issue.}

\tightsubsection{Evaluation of Quality Improvement}
\label{subsec:go-improve}

We use two metrics to measure the quality improvement over a set of
sessions: improvement ratio and optimality, defined as follows:

\begin{align*}
& ImprovementRatio=\frac{Q_{GO}-Q_{Baseline}}{Q_{Baseline}}\\
& Optimality=\frac{Q_{Oracle}-Q_{Baseline}}{Q_{GO}-Q_{Baseline}},
\end{align*}

\noindent
where $Q$ is the average quality across all sessions. Recall that an
oracle would select the best decision based on its true outcome, which
is only available in augmented trace-driven inputs and controlled
synthetic inputs.  By default, we use random selection as the baseline
which always selects a uniformly random decision among all possible
ones. The higher the improvement ratio, the better, and, similarly,
the closer the optimality is to one the better.

We use the counterfactual input method to evaluate GO's improvement
ratio. Figure~\ref{fig:cross-metrics} shows the improvement ratio on
each metric when GO uses different metrics as utility functions. We
say that GO uses metric $m$ as an utility function when it aims to
improve $m$, i.e., GO makes decisions based on prediting $m$'s quality
value. Not surprisingly, the largest improvement for each metric
corresponds to the case in which GO uses that metric as the utility
function. However, surprisingly, we also see improvements for metrics
when they are not used as utility functions. For example, by using
metrics other than average bitrate as utility function, we can still
achieve 2\%-25\% improvements for the average bitrate. This is because
the quality metrics are in general correlated with the CDN. For
example, if CDN $A$ provides a higher bitrate than CDN $B$, it will
also provide a lower buffering ratio or lower join time than $B$.


\begin{figure}[h!]
\centering
 \includegraphics[width=0.45\textwidth] {figures/newfig/cross-metric.pdf}
\tightcaption{Improvement ratio on each metric by using different metric as utility function.}
\label{fig:cross-metrics}
\end{figure}


\tightsubsection{Sensitivity Analysis}

\tightsubsubsection{Impact of Prediction Accuracy}
\label{subsec:impact-accuracy}

%To quantify the impact of prediction accuracy, we use the augmented
%trace-driven as it provides us the actual quality which is needed to
%to quantify the prediction error. We change GO's prediction error and
%evaluate the improvement ratio and optimality.

We use the augmented trace-driven testing to control the prediction
error and quantify its impact on the quality improvement. Given a
session and a decision, let $q$ be the actual quality and let $p$ be
the predicted quality for that session and decision. We then use a
stretch parameter $r_a$ to control the prediction error by changing
the predicted quality from $p$ to $p'=q+(p-q)r_a$.  We compare then
the quality improvement with an oracle which always makes the best
decision. To simplify the discussion, we assume we can make two
distinct decisons for each session.

\begin{figure}[h!]
\centering
 \includegraphics[width=0.35\textwidth] {figures/newfig/trendAccuracy-metricId1-keyGlobal-partition.pdf}
\tightcaption{Impact of GO prediction accuracy with varying stretch ratio $r_a$ on quality improvement.}
\label{fig:trace-accuracy-2}
\end{figure}

Figure~\ref{fig:trace-accuracy-2} shows GO's improvement ratio when
average bitrate is used both as utility function and evaluation
metric.  It shows a clear degredation of the improvement ratio as the
strecth parameter, $r_a$ increases, and, consequently as the predicted
error increases. When stretch ratio is close to zero, GO achieves
perfect prediction matching the oracle (i.e., GO's optimality is
one). 
%With stretch ratio larger than 0.7, every 10\% decrease in the
%prediction error leads to a 20Kbp increase in the improvement ratio of
%average bitrate. 
These results illustrate that, as expected, the quality improvement is
sensitive to accuracy. 
%For example, if the prediction error is reduced
%by 30\%, GO will achieve 12.5\% better quality improvement.
%Also, note that with smaller stretch rate, the improvement stablizes as the improvement converges to oracle improvement with accurate prediction.



\tightsubsubsection{Impact of Quality Diversity}
\label{subsec:impact-diversity}

To quantify the impact of quality diversity among multiple decisions,
we fix the prediction error given by GO, and use a stretch ratio $r_b$
to control the quality diversity between multiple decisions. To
simplify the discussion, we again assume we can only make two distinct
decisions for each session. Let $q_1, q_2$ be the actual quality
values corresponding to the two decisions for a given
session. Assuming $q_1\leq q_2$, we change these actual quality value
to $q_1'=q_1, q_2=q_1+(q_2-q_1)r_b$. We show quality improvement in
absolute number over the baseline algorithm, as well as the
optimality.

%Intuitively, it is expected that with larger quality diversity among multiple decisions, we should see that GO's performance should be closer to the oracle approach since the gap between decisions' outcome is so large that the best decision will be made with any prediction error. 

\begin{figure}[h!]
\centering
\subfigure[Average bitrate (Kbps)]
{
        \includegraphics[width=0.24\textwidth]{figures/newfig/trendDiversity-metricId1-keyGlobal-partition.pdf}
}
\hspace{-0.6cm}
\subfigure[Join time (ms)]
{
        \includegraphics[width=0.24\textwidth]{figures/newfig/trendDiversity-metricId2-keyGlobal-partition.pdf}
}

\tightcaption{Impact of stretch of quality diversity $r_b$ on quality improvement. Green lines with values on the left y axis show GO's improvement over the randomized baseline and red lines with values on the right y axis show optimality (GO's improvement/oracle's improvement)}
\label{fig:trace-diversity}
\end{figure}

Figure~\ref{fig:trace-diversity} shows the results when the average
bitrate and join time are used as utility functions as well as
evaluation metrics. Both figures show a close-to-linear relationship
between the improvement and the quality diversity across decision
outcomes.  In addition, both figures show that when the diversity
approaches 100\%, GO approaches the performance of the oracle
approach.



\tightsubsection{Summary of Findings}
\begin{packedenumerate}
	\item Under controlled experiments, GO identifies performance change and switches decisions accordingly.
	\item GO achieves noticeable improvement on four metrics, e.g., for buffering ratio, 1.5\% globally and as much as 4.8\% for some popular site.
	\item Optimizing for one metric does not impact other metric negatively. Instead, we see improvement on all metrics if only one metric is used for decision selection.
	\item Quality improvement is linear to prediction error -- 10\% reduction in prediction error yields to 20Kbps in average bitrate improvement, and if current prediction error is reduced by 30\%, GO will have 12.5\% more improvement in average bitrate.
	\item GO's quality improvement in average bitrate increases almost linearly with quality diversity between decisions, and such improvement becomes closer-to-optimal with larger quality diversity -- if the current quality difference increases by 50\%, GO can achieve improvement in join time as much as 90\% of the optimal improvement.
\end{packedenumerate}


