\tightsection{Quality Improvement via Prediction}
\label{sec:improvement}

Previous sections have established our prediction algorithm can achieve near-optimal accuracy. In this section, we empirically study the relationship between prediction accuracy and video quality improvement, i.e., does a good prediction lead to high improvement. 

Specifically, we aim to answer two questions:
\begin{packedenumerate}
	\item What quality improvement does GO provide under various scenarios?
	\item How does prediction accuracy impact quality improvement in real world?
\end{packedenumerate}

\tightsubsection{Methodology}
We study these questions by using offline analysis methodologies including trace-driven analysis. As we will introduce in next sections, we have built a prototyping system that runs in real world. However, we believe these questions should be better addressed using offline trace-driven analysis because the real world experiments has four limitations. We will also use the real world expriments (especially, using A/B testing methods) to demonstrate GO's behavior and its quality improvement in the wild in \Section~\ref{sec:eval}.
\begin{packedenumerate}
	\item Limited coverage: As we will explain, the real world experiments only have access to the video traffic of one video site on one type of OS. The stricts its coverage of classes of traffic in both video genres and streaming protocol.
	\item Limited scale of simultaneous expriments: Since we only control one site's traffic in real world experiments, it is not feasible to compare many configurations or algorithms in parallel (empirically, at most 3-4) and ensure each of them receive sufficient traffic to draw confident conclusions.
	\item Limited diversity in performance: As we will see, the improvement in quality is greatly impacted by the diversity and variability of the performance outcome of different decisions (e.g., different CDNs). However, we can only observe a small number of possible patterns in performance diversity.
	\item Limited controlled experiments: Finally, it is impossible to conduct sensitivity analysis (as in this section) where we would like to see the impact of changing one parameter (e.g., CDN performance) at a time with controlled amount.
\end{packedenumerate}


This section uses three input to answer the two high-level questions. We now describe them in the order of closeness to realism and in reverse order of degree of controllability.

\myparatight{Counterfactual input} This approach is the closest to real-world A/B testing. The basic idea is that we first collect a dataset in which the decisions are randomly made for each client (called {\it random dataset}) and collect the quality metrics of each video session. Then we evaluate a given decision algorithm by picking the sessions for which the decision the algorithm makes matches the decisions in the random dataset. In \Section~\ref{sec:counterfactualtesting}, we show that this approach is unbiased. This methodology can be applied on video traffic we collect but GO currently do not optimize, thus giving us a bigger picture and better answer the above-mentioned questions. \jc{Xi, what does this sentence mean?}

\myparatight{Augmented trace-driven input} While counterfactual input is useful in evaluating different algorithms, we cannot use it to estimate the ``would-be" performance of an ``oracle" approach (e.g., how the performance would be if another decision were taken), which requires data extrapolation. To remedy these limitations, we generate an augmented trace-driven synthetic dataset in which we extrapolate for each session the ``true'' quality outcome of all its possible decisions. The basic idea of this extrapolation  is that for each session $s$, its outcome of each decision $d$ is drawn from the distribution of outcomes of this decision $d$ on other sessions exactly matching the same attributes with $s$. Having ensured that quality outcomes in the synthetic scenario are known for any decision, it is possible to identify an {\it oracle} approach that always makes the best decision.

\myparatight{Controlled synthetic input} Both previous methodology lacks control, i.e., if no major outage or other events happen in the data set, we cannot evaluate GO performance under those scenarios. \xil{the following is hard to read.} Thus we also use fully controlleable input in which the true outcomes (i.e., ``ground truth'') of each decision is controlled.





\tightsubsection{Behavioral study}
We first explain the GO's prediction-based decision making and use controlled synthetic input to confirm its behavior -- it will 

\myparatight{Prediction-based dececision making} GO's decision algorithm simply selects for each session under prediction the decision that has the best predicted quality in a predetermined metric (e.g., buffering ratio). Note that GO always leaves a small fraction of traffic to be randomly allocated to guarantee that each decision will have at least certain fraction of traffic even when that decision is not the best.




