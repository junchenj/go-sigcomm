\tightsection{Real World Evaluation}
\label{sec:eval}

In this section, we present the implementation and deployment of the GO system (\Section~\ref{sec:impl} and ~\ref{subsec:eval_setup}), and evaluate its performance in real-world (\Section~\ref{subsec:real-world-improvement} and ~\ref{subsec:adaptive-bitrate}). 

The overall improvement is not impressive partly because the deployment is greatly constrained. The deployment only uses one content provider, and the content provider only enables CDN-level selection across three major CDNs which exhibit limited difference in their overall performance during the cause of our experiment. Moreover, the optimization must follow a traffic policy which dictates the total amount of traffic to each CDN. \jc{add a sentence here to explain how GO makes predictive selection under the policy constraints}

However, even with these constraints, we still see arguable improvement. 

\begin{packedenumerate}
    \item We see an overall 7\% reduction in buffering ratio, 4\% increase in average bitrate, and 16\% reduction in number of bitrate switches compared with a random baseline selection algorithm.
    \item We see buffering raito reduction of more than 20\% in several days when a major CDN is experiencing performance degradation.
    \item We show that the initial bitrate chosen by GO is 19\% closer to the dominant bitrate.
\end{packedenumerate}




\begin{figure*}[t!]
\centering
\subfigure[Daily improvement (both metrics) \jc{Xi , use the first 10 days only}]
{
  \includegraphics[width=0.3\textwidth] {figures/eval-perfimp.pdf}
  \label{subfig:buffering-and-bitrate}
}
\subfigure[Buffering ratio (\%): GO vs. per-CDN]
{
	\includegraphics[width=0.3\textwidth]{figures/ab-testing-figures/bufferingratio-new.pdf}
	\label{subfig:eval-case-study:bufferingratio}
}
\subfigure[Avg. bitrate (Kbps): GO vs. per-CDN]
{
	\includegraphics[width=0.3\textwidth]{figures/ab-testing-figures/averagebitrate-new.pdf}
	\label{subfig:eval-case-study:averagebitrate}
}
\tightcaption{Performance Metrics and Improvement from GO}
\label{fig:perf-impr}
\end{figure*}



\tightsubsection{System Implementation \& Scalability}
\label{sec:impl}
We first present the high level GO implementation. While there are much details in building GO system, we focus on the two main components: data processing and decision making. Paramount in our implementation was the need to make very rapid decisions, as any extra delay in decision making could impact performance of the client making the request. This meant that the vast majority of prediction information needed to be precomputed. This resulted in an implementation that involves two parts: computation and updating of a group table every minute, and broadcasting of the table out to servers close to the clients.

\myparatight{Creating group table} Group table generation is essentially the process of aggregating quality samples into partitions of different ACs, and then computing summary statistics for each partition (e.g., mean and standard error of mean).
Our implementation uses Spark~\cite{spark} as the underlying compute framework, and the problem can be captured in a single map-reduce stage. 
Quality samples are loaded in parallel from an HDFS source~\cite{hadoop} and distributed amongst the cluster in form of Resilient Distributed Datasets (RDD)~\cite{zaharia2012resilient}. 
%Each node then maps its share of quality samples to a set of preliminary buckets. Common buckets are then shuffled across the cluster and the results are aggregated and collected onto a master machine. 
Currently it takes GO system 12 seconds to process 500K partitions on a 4-node cluster with \fillme cores and \fillme memory \jc{Xi, fill in the numbers}, without riguous performance optimization.
It is also worth noting that all steps involved in creating the table are horizontally scalable.

\jc{Hui, remember to add a para on policy constraints}

\myparatight{Broadcasting group table} The resulting table is broadcasted wholesale out to a large number of decision makers (DMs) closer to video clients in multiple POPs by way of a distributed messaging system~\cite{kreps2011kafka}. The bottleneck here is bandwidth, because at minimum, we need to ship one table copy to each POP where we deployed decision makers. We believe this is justified by providing quick response to video players. Our current GO table size is 50MB for 500K entries, with an update frequency of one minute, the bandwidth required is 0.83Mbps per POP. We would like to note that we have not optimized serialization and deserialization in terms of size.

\myparatight{Decision making} Once a decision maker has a table, decision making is a matter of running the prediction algorithm for evaluation of each decision. Given that there are $N$ ACs for decision evaluation, and $M$ possible decisions, there are $M \times N$ lookups for a decision to be made. The decision making process is horizontally scalable by adding more decision makers, and in general, decision making is extremely fast. The average response time from current GO system is $0.62 ^{+}_{-} 0.016$ms with 3 decision makers (both query per second and CPU utilization are low on the decision makers), which is insignificant compared to RTT in Internet.

In summary, we implemented GO which is both scalable and provide good system-level performance.


\tightsubsection{Deployment Setup}
\label{subsec:eval_setup}

We evaluate the performance of GO on one content provider with short-form videos (5 minutes) on iOS device (both iPhone and iPad). 
HLS protocol is running on the iOS device and our GO system decides the initial CDN and bitrate, while Apple player employs adaptive 
bitrate switching algorithm to change the bitrate during the video session (without CDN switch). 

The results are collected from Jan 1st, 2014 to Jan 25th, 2014 (total of 25 days) with 26.1 million sessions (870K sessions on average per day).
There are three CDNs in this experiement, and 5 bitrate levels (from 700Kbps to 3.5Mbps). We evaluate two algorithms: {\it randomized} baseline and {\it optimized} (by GO).


\tightsubsection{Quality Improvement}
\label{subsec:real-world-improvement}

\jc{
\begin{packeditemize}
	\item Use Figure~\ref{subfig:buffering-and-bitrate} to show consistent improvement over time
	\item Use Figure~\ref{subfig:eval-case-study:bufferingratio} and~\ref{subfig:eval-case-study:averagebitrate} to show that GO outperform best CDN's overall performance, suggesting that GO is assigning traffic adaptively.
\end{packeditemize}
}

Figure~\ref{subfig:buffering-and-bitrate} shows the performance improvement of GO over randomized baseline on two quality metrics of
buffering rate and average bitrate over the 25-day period. The overall improvement over the entire 25-day period is 7\% reduction in buffering ratio and 4\% increase in 
average bitrate. The figure also shows that performance improvement varies by day, depending on how different the performance of each CDN. 
In fact, we noticed that, for this site, performance of multiple CDNs tend to become close to each other
during weekends where CDNs are likely under-loaded and thus, we see no noticeable improvement from GO in those days. 
On the other hand, on day 25, we noticed a big performance improvement of more than 20\%. After investigation,
we found that a major CDN was experiencing performance issues that lasts for more than one day. 

To further illustrate GO's ability to handle spatial and temporal variation, 
we look into some particular examples and compare GO with each CDN. Figure~\ref{subfig:eval-case-study:bufferingratio} and~\ref{subfig:eval-case-study:averagebitrate} show two example days in which three CDNs performed differently.  For example, in terms of buffering ratio, CDN3 was the best on Day A and CDN1 was the best on Day B. 
We can see that in both Day A and B, GO managed to outperform the best-performing CDN in both average bitrate 
and buffering ratio. This suggests that GO was not only looking for the globally best CDN, but also 
differentiated CDN performance in finer granular partition.


\tightsubsection{Adaptive Bitrate Behavior}
\label{subsec:adaptive-bitrate}

Previous research (e.g., ~\cite{user-adaptive,videoqoe}) shows that bitrate switching rate has an impact on video experience. In this section, we evaluate the adaptive bitrate switching behavior with and without GO. 

\jc{We show that (1) GO can reduce the number of bitrate switches (Figure ~\ref{subfig:reduce-switch}) by (2) selecting the initial bitrate to be close to the ``stablized'' bitrate (Figure ~\ref{subfig:initvsdom})}.

\comment{
\myparatight{When Do Adaptive Bitrate Switches Happen?}
We first take a look at when do adaptive bitrate switches happen. Figure~\ref{fig:switch-time-dist} shows that the 
majority of switches happen at the beginning of the video plays, i.e., around 60\% of the switches 
happen within 20\% of the video duration. Switching behavior varies for different adaptive bitrate algorithms, and we do not 
claim such observation apply to all algorithms.

To account for viewers leaving in the middle of the video play, note that the x-axis is chosen as the percentage of actual 
video play duration instead of content length. 
In this figure, we include another content provider who provides long-formed content (one hour) on iOS platforms, and we have similar observations. This suggests the importance of initial bitrate selection, through which viewers can have a smoother video playback experience.

\begin{figure}[h!]
\centering
 \includegraphics[width=0.3\textwidth] {figures/switch-time-dist.pdf}
\tightcaption{When do adaptive bitrate switches happen? \jc{what's the threshold between long and short form content}}
\label{fig:switch-time-dist}
\end{figure}
}


\myparatight{Reducing number of  bitrate switch}
We evaluate the switching stability by looking at the number of switches within the first minute of the video play.
We chose one minute because the videos are 5 minutes long and first minute account for 60\% of the switches (see Figure~\ref{fig:switch-time-dist}). 
Since HLS has 10 second video chunks, the maximum number of switches is 6 in this case.
Figure~\ref{subfig:reduce-switch} shows the number of bitrate switches for optimized and random decisions. 
Overall GO can reduce the number of switches by 16\%, and it also increases the number of non-switching-interrupted sessions 
(sessions that experience no switching) by 10\%.
It is possible the reason GO reduces the number of switches is shorter video plays, however, we observed engagement lift
(viewers watch longer because of better quality) at the same time of reduction in number of switches.



\begin{figure}[h!]
\centering
\subfigure[Number of bitrate switches \jc{Xi, revert to last version?}]
{
  \includegraphics[width=0.24\textwidth] {figures/eval-reduceswitch.pdf}
  \label{subfig:reduce-switch}
}
\hspace{-0.6cm}
\subfigure[Initial vs. dominant bitrate]
{
  \includegraphics[width=0.24\textwidth] {figures/eval-initvsdom.pdf}
  \label{subfig:initvsdom}
}
\tightcaption{GO improves bitrate adaptation by selecting a better initial bitrate and thus reducing number of bitrate switches.}
\label{fig:bitrate-stability}
\end{figure}


\myparatight{Dominant vs. Initial Bitrate}
Another metric we use to evaluate GO initial bitrate selection is the rate between dominant bitrate and initial bitrate. Dominant bitrate
is the bitrate that the sessions plays for the longest duration, and ideally this number should be 1. Figure~\ref{subfig:initvsdom} shows the ratio of initial bitrate to dominant bitrate. Overall GO is 19\% closer to the dominant bitrate than randomized bitrate selection. 
The figure also shows that for 60\% of the GO-optimized sessions, the initial bitrate selected by GO is almost the same as the dominant bitrate.
%We would like to emphasize again that GO does not make the improvement at the price of engagement, and GO in fact improves engagement.

In summary, our early experience of GO in the real world suggests that it is a promising step towards the global control plane.
