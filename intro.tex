\tightsection{Introduction}

\comment{
\begin{packeditemize}
	\item Increasing demands for high QoS and performance.
	\item Limitation of current tenent of Internet/application protocols.
	\item Quality depends on prediction accuracy.
	\item Accurate prediction for video quality is possible through sharing clients' information.
	\item Sharing clients' information is feasible.
	\item Contribution of this paper.
\end{packeditemize}
}


Today's Internet applications are demanding better-and-better Quality of Service (QoS) and performance. Examples of such applications include video streaming, hosted productivity applications, social networks, and on-line multiplayer games. These requirements will only increase in the future with more and more applications and services being hosted, and an ever increasing demand for higher quality video (e.g., 4K video).

A tenet of today's Internet protocols such as TCP---which represents the vast majority of the Internet traffic---and various media/video protocols is to rapidly react to congestion and/or changes in the resource availability along the path or at end-hosts. While these reactive protocols have served us well for decades, they have two inherent limitations that make them ill-suited for the ever increasing demand for better quality of service: 

\myparatight{Suboptimal initial configurations} many protocols use static configuration parameters that are often suboptimal. For example, in many cases, the initial window size of TCP is too small, which may cause a transfer to take far more RTTs than necessary. In another example, adaptive streaming protocols typically use a statically configured bitrate. However, if this bitrate is too low, the protocol might not be able to reach the optimal rate by the time the video has ended (e.g., for a 30s or 60s news clip). 

\myparatight{Suboptimal decisions} when these protocols react, they don't always make the optimal decisions which may further impact the user experience. For example, TCP may use a retransmission timeout that is way too conservative, or a bitrate switching protocol may switch down to a rate that is still too high for the given network conditions.

%\myparatight{Suboptimal decisions:} by the time these protocols react, it might be already too late to mask the quality problems to users. For example, if the TCP loses a packet in the initial window, this will trigger a few seconds retransmission timeout that will impact the user experience. 

In this paper we make two arguments. First, we argue that to address these challenges one needs to accurately predict the outcome of making a particular choice, e.g., would a stream be able to sustain a particular bitrate? Would a TCP connection experience any loss given a particular initial window size? In theory, this would allow protocols to use ``optimal'' configuration parameters and make ``optimal'' decisions, e.g., pick the largest sustainable bitrate for a video stream,  or pick the largest window size for which a TCP connection won't experience congestion losses. 

Second, we argue that to accurately predict the outcome of a given choice, we need to leverage the information available from other streams or connections, i.e., use the performance experienced by other ``similar'' sessions to predict the performance of another session. For example, we would expect that if multiple sessions located at organization X can sustain 2Mbps when streaming from CDN Y, then a new session from X will be also able to stream at 2Mbps from CDN Y. 

To this end, we propose a global control plane architecture that continuously collects information about the performance of existing sessions and use this information to maximize performance of other sessions. There are several challenges to implement such an architecture. First, we need the ability to collect information from a large number of streams. Second, we need to process and make decisions based on this aggregate information in real-time. Third, to make these decisions, we need to model the network and distribution environments accurately. 

In this paper, we present an instantiation of such control plan architecture for video streaming, called Video Global Optimization (GO), which addresses the above challenges.  Video traffic is singularly important as it dominates the Internet traffic today, and this domination will only intensify in the forseable future. GO instruments each client streaming video to send quality related information (e.g., current bitrate, re-buffering, start time) to a backend. This backend process the quality information in real-time and based on this information provide hints to clients about the best bitrate or CDN to start with or switch to. We present the system architecture, as well as discuss in detail the challenges and the solution for making accurate predictions based on the aggregate quality information received by the backend. 

....

{\bf [Ion: The rest should go into background; also the background should be split; Also sub-section 3.3 should be a standalone section.]}

How are protocols designed?  To somewhat oversimplify: The designer implicitly or explicitly builds a model of the world, from which predictions about outcomes of decisions are made.  Then the protocol takes the decision that optimizes the utility of the predicted outcome.  For example, if it were possible to perfectly predict whether packet loss would occur given a particular TCP initial window size, we could maximize the QoS and user experience by choosing the highest initial window size that wouldn't cause a packet loss.  Similarly, if we could perfectly predict whether a client could sustain a particular bitrate, we could choose the highest sustainable bitrate.

Of course, modeling the world is difficult, and often we must use information from the real world to inform our models.  When is data-driven modeling important?  Generically, using data is helpful if our decisions are important (since otherwise we don't stand to gain much through better modeling) and the world is complex enough that we cannot model it well otherwise \henry{how complex? and why such complexity makes model insufficient?}.

We argue that protocol design is no different: We can accurately predict the outcomes of protocol decisions by sharing the clients' information. The main idea is to use the performance of existing clients to predict the performance of another client. For example, when a new client starts streaming a video, we can use the quality experienced by other clients to predict the quality that would be experienced by the new client when streaming at a given bitrate. Based on this prediction, we can then select the optimal bitrate for the new client.  In TCP protocol design, we might gather data about many TCP connections to discover the distribution of packet loss rates for different initial window sizes.  \jc{Also cite sigcomm'12 paper on data-driven TCP design which argues that TCP parameters need to be driven by real data}

We refer to the resulting model\henry{need a different word for this; ``model'' is now unfortunately overloaded} of sharing information across clients and making decisions based on predictions made by shared information as a predictive decision model. We note that predictive decision model which requires sharing the information across clients at a global scale is naturally enabled by today's service architectures where a very large number of clients are typically connected to a backend at any given time. Google, Facebook, Microsoft, Yahoo!, and Twitter are just a handful of sites that can observe the quality and the performance experienced by a huge number users at any give time. Thus, maximizing the QoS of existing applications using predictive decision model does not require new architectures or building new large scale systems; it can be done in the context of the already existing artifacts.  However, due to the complexity of networks -- in particular their temporal and spatial heterogeneity -- we find it is necessary to bring a degree of statistical sophistication to network modeling algorithms.

In this paper, we apply the idea of predictive decision model to video quality optimization in the context of a large site that manages the video delivery of many premium content brands such as HBO, MOL, and ESPN. Video traffic is singularly important as it dominates the Internet traffic today, and this domination will only intensify in the forseable future. We describe a solution architecture, called Video Global Optimization (GO) and the associated algorithms to collect and use real-time quality-related information from every client currently streaming video to predict the quality of another user if she were streaming from a particular CDN at a particular bittrate, and use this prediction to select initial bit rate and CDN for a new user.

\ion{... something on how this is cross-layer SDN...}
