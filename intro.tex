\tightsection{Introduction}
\label{sec:intro}

\comment{
\begin{packeditemize}
	\item Increasing demands for high QoS and performance.
	\item Limitation of current tenent of Internet/application protocols.
	\item Quality depends on prediction accuracy.
	\item Accurate prediction for video quality is possible through sharing clients' information.
	\item Sharing clients' information is feasible.
	\item Contribution of this paper.
\end{packeditemize}
}


Today's Internet applications are demanding better-and-better Quality of Service (QoS) and performance. Examples of such applications include video streaming, hosted productivity applications, social networks, and on-line multiplayer games. These requirements will only increase in the future with more and more applications and services being hosted, and an ever increasing demand for higher quality video (e.g., 4K video).

A tenet of today's Internet protocols such as TCP---which represents the vast majority of the Internet traffic---and various media/video protocols is to rapidly react to congestion and/or changes in the resource availability along the path or at the end-hosts. While these reactive protocols have served us well for decades, they have three inherent limitations that make them ill-suited for the ever increasing demand for better quality of service: 

\myparatight{Suboptimal initial configurations} existing protocols use typically \emph{static} configuration parameters that are often suboptimal. For example, in many cases, the initial window size of TCP is too small, which may cause a transfer to take far more RTTs than necessary. Similarly, adaptive streaming protocols usually start with a statically configured bitrate. If this bitrate is too low, the protocol might not be even able to reach the optimal rate by the time the video has ended (e.g., for a 30s or 60s news clip)!

\myparatight{Suboptimal decisions} when these protocols react, they don't always make the optimal decisions which may further impact the user experience. For example, TCP may use a retransmission timeout that is way too conservative, or a bitrate switching protocol may switch down to a rate that is still too high for the given network conditions.

\myparatight{Large configuration space} as protocols and applications become more sophisticated the number of configuration choices increases dramatically. For instance, with a video application one can select the initial bit-rate, the CDN, and at a finer granularity the proxy or the web server from which to stream the content. This makes it hard, or even infeasible, for a reactive protocol to explore the configuration space and select the best configuration.

%\myparatight{Suboptimal decisions:} by the time these protocols react, it might be already too late to mask the quality problems to users. For example, if the TCP loses a packet in the initial window, this will trigger a few seconds retransmission timeout that will impact the user experience. 

%In this paper we make three arguments. First, we argue that to address these challenges one needs to accurately predict the outcome of making a particular choice, e.g., would a stream be able to sustain a particular bitrate? Would a TCP connection experience any loss given a particular initial window size? In theory, this would allow protocols to use ``optimal'' configuration parameters and make ``optimal'' decisions, e.g., pick the largest sustainable bitrate for a video stream,  or pick the largest window size for which a TCP connection won't experience congestion losses.

In this paper we make three arguments. First, given the fundamental limitations of reactive approaches, we argue for the alternative \emph{predictive} approach, where we need to accurately predict the outcome of making a particular choice, e.g., would a stream be able to sustain a particular bitrate? Would a TCP connection experience any loss given a particular initial window size? In theory, this would allow protocols to use ``optimal'' configuration parameters and make ``optimal'' decisions, e.g., pick the largest sustainable bitrate for a video stream,  or pick the largest window size for which a TCP connection won't experience congestion losses.

Second, in order to accurately predict the outcome of a given choice, one may be tempted to use the analytical approach to model the environment or Internet. However, we believe this is infeasible. Thus we argue for a \emph{data-driven} empirical approach to leverage the information available from other streams or connections, i.e., use the performance experienced by other ``similar'' sessions to predict the performance of another session. For example, if multiple sessions located at some organization can sustain 2Mbps when streaming from a CDN, then it's likely a new session from the same organization will be also able to stream at 2Mbps from the same CDN.

Third, given the large number of attributes that impact a session performance (e.g., protocol, device, OS, firmware, ISP, geographic location, to name a few) even if we have access to a very large number of sessions, it is hard to find enough sessions that match all the attributes of the session whose performance we wish to predict. To address this challenge we argue that we need to \emph{aggregate} sessions with similar but not exact attributes, and that the attributes along we perform the aggregation are determinant in improving the prediction accuracy.
\xil{the last sentence is a bit vague}

To this end, we propose a global control plane architecture that continuously collects information about the performance of existing sessions and use this information to maximize performance of other sessions. There are several challenges to implement such an architecture. First, we need the ability to collect information from a large number of streams. Second, we need to process this information and make decisions in real-time. Third, to make these decisions, we need to accurately model the session performance. 


In this paper, we present an early instantiation of such control plane architecture for video streaming, called Video Global Optimization (GO), which addresses the above challenges.  Video traffic is singularly important as it dominates the Internet traffic today, and this domination will only intensify in the forseable future. GO instruments each client streaming video to send quality related information (e.g., current bitrate, re-buffering, start time) to a backend. This backend process the quality information in real-time and based on this information provide hints to clients about the best bitrate or CDN to start with or switch to. We present the system architecture, as well as discuss in detail the challenges and the solution for making accurate predictions based on the aggregate quality information. Our early experience deploying GO to optimze video streaming from one site shows that it is a promising first step towards the full global control plane. 
   
The rest of the paper is organized as follows. Section~\ref{sec:overview} presents the motivation and overview of GO. In Section~\ref{predictability}, we will answer the question of how predictable are video quality metrics. Section~\ref{sec:challenges} shows the challenges of designing a prediction algorithm and we present our solution in Section~\ref{sec:prediction}. We evaluate GO performance in Section~\ref{sec:improvement}. In Section~\ref{sec:impl}, we show the implementation and we share some early experience in Section~\ref{sec:eval}.
\xil{one high level comment is that we are mentioning TCP a lot, but we have nothing to follow on. Maybe we can change the tune a bit to just have video as the main thread, and explicitly spell out TCP or SDN as analogy}
