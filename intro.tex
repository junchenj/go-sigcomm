\tightsection{Introduction}

\comment{
\begin{packeditemize}
	\item Increasing demands for high QoS and performance.
	\item Limitation of current tenent of Internet/application protocols.
	\item Quality depends on prediction accuracy.
	\item Accurate prediction for video quality is possible through sharing clients' information.
	\item Sharing clients' information is feasible.
	\item Contribution of this paper.
\end{packeditemize}
}


Today's Internet applications are demanding better-and-better Quality of Service (QoS) and performance. Examples of such applications include video streaming, hosted productivity applications, social networks, and on-line multiplayer games. These requirements will only increase in the future with more and more applications and services being hosted, and an ever increasing demand for higher quality video (e.g., 4K video).

A tenet of today's Internet protocols such as TCP---which represents the vast majority of the Internet traffic---and various media/video protocols is to rapidly react to congestion and/or changes in the resource availability along the path or at end-hosts. While these reactive protocols have served us well for decades, they have two inherent limitations that make them ill-suited for the ever increasing demand for better quality of service: 

\myparatight{slow reaction:} by the time these protocols react, it might be already too late to mask the quality problems to users. For example, if the TCP loses a packet in the initial window, this will trigger a few seconds retransmission timeout that will impact the user experience. In another example, a bitrate switching algorithm that starts streaming at a rate which it is too low, might not be able to reach the optimal rate by the time the video has ended (e.g., for a 30s or 60s news clip). This will again compromise the user experience.

\myparatight{suboptimal decisions:} when these protocols react, they don't always make the optimal decisions which may further impact the user experience. For example, TCP may use a retransmission timeout that is way too conservative, or bitrate switching protocol may switch down to a rate that is still too high for the given network conditions.

How are protocols designed?  To somewhat oversimplify: The designer implicitly or explicitly builds a model of the world, from which predictions about outcomes of decisions are made.  Then the protocol takes the decision that optimizes the utility of the predicted outcome.  For example, if it were possible to perfectly predict whether packet loss would occur given a particular TCP initial window size, we could maximize the QoS and user experience by choosing the highest initial window size that wouldn't cause a packet loss.  Similarly, if we could perfectly predict whether a client could sustain a particular bitrate, we could choose the highest sustainable bitrate.

Of course, modeling the world is difficult, and often we must use information from the real world to inform our models.  When is data-driven modeling important?  Generically, using data is helpful if our decisions are important (since otherwise we don't stand to gain much through better modeling) and the world is complex enough that we cannot model it well otherwise \henry{how complex? and why such complexity makes model insufficient?}.

We argue that protocol design is no different: We can accurately predict the outcomes of protocol decisions by sharing the clients' information. The main idea is to use the performance of existing clients to predict the performance of another client. For example, when a new client starts streaming a video, we can use the quality experienced by other clients to predict the quality that would be experienced by the new client when streaming at a given bitrate. Based on this prediction, we can then select the optimal bitrate for the new client.  In TCP protocol design, we might gather data about many TCP connections to discover the distribution of packet loss rates for different initial window sizes.  \jc{Also cite sigcomm'12 paper on data-driven TCP design which argues that TCP parameters need to be driven by real data}

We refer to the resulting model\henry{need a different word for this; ``model'' is now unfortunately overloaded} of sharing information across clients and making decisions based on predictions made by shared information as a predictive decision model. We note that predictive decision model which requires sharing the information across clients at a global scale is naturally enabled by today's service architectures where a very large number of clients are typically connected to a backend at any given time. Google, Facebook, Microsoft, Yahoo!, and Twitter are just a handful of sites that can observe the quality and the performance experienced by a huge number users at any give time. Thus, maximizing the QoS of existing applications using predictive decision model does not require new architectures or building new large scale systems; it can be done in the context of the already existing artifacts.  However, due to the complexity of networks -- in particular their temporal and spatial heterogeneity -- we find it is necessary to bring a degree of statistical sophistication to network modeling algorithms.

In this paper, we apply the idea of predictive decision model to video quality optimization in the context of a large site that manages the video delivery of many premium content brands such as HBO, MOL, and ESPN. Video traffic is singularly important as it dominates the Internet traffic today, and this domination will only intensify in the forseable future. We describe a solution architecture, called Video Global Optimization (GO) and the associated algorithms to collect and use real-time quality-related information from every client currently streaming video to predict the quality of another user if she were streaming from a particular CDN at a particular bittrate, and use this prediction to select initial bit rate and CDN for a new user.

\ion{... something on how this is cross-layer SDN...}
